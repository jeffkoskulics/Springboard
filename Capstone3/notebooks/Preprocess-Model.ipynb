{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "In this step, we convert the raw text into numerical features for analysis. We have to convert both the keywords and text data. Let's start with the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>4368</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>Nepal earthquake 3 months on: Women fear abuse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>2216</td>\n",
       "      <td>chemical%20emergency</td>\n",
       "      <td></td>\n",
       "      <td>THE CHEMICAL BROTHERS to play The Armory in SF...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>1816</td>\n",
       "      <td>buildings%20on%20fire</td>\n",
       "      <td>World Wide</td>\n",
       "      <td>1943: Poland - work party prisoners in the Naz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>2932</td>\n",
       "      <td>danger</td>\n",
       "      <td>Hailing from Dayton</td>\n",
       "      <td>I wish I could get Victoria's Secret on front....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>6553</td>\n",
       "      <td>injury</td>\n",
       "      <td></td>\n",
       "      <td>DAL News: Wednesday's injury report: RB Lance ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                keyword              location  \\\n",
       "3044  4368             earthquake  Melbourne, Australia   \n",
       "1534  2216   chemical%20emergency                         \n",
       "1260  1816  buildings%20on%20fire            World Wide   \n",
       "2042  2932                 danger  Hailing from Dayton    \n",
       "4610  6553                 injury                         \n",
       "\n",
       "                                                   text  target  \n",
       "3044  Nepal earthquake 3 months on: Women fear abuse...       1  \n",
       "1534  THE CHEMICAL BROTHERS to play The Armory in SF...       0  \n",
       "1260  1943: Poland - work party prisoners in the Naz...       1  \n",
       "2042  I wish I could get Victoria's Secret on front....       0  \n",
       "4610  DAL News: Wednesday's injury report: RB Lance ...       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../downloads/train.csv')\n",
    "test = pd.read_csv('../downloads/train.csv')\n",
    "\n",
    "train.keyword.fillna('',inplace=True)\n",
    "test.keyword.fillna('',inplace=True)\n",
    "train.location.fillna('',inplace=True)\n",
    "test.location.fillna('',inplace=True)\n",
    "\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=tt.tokenize)\n",
    "cv.fit(train['text'])\n",
    "\n",
    "# Vectorize keywords and tweets\n",
    "# First the training set\n",
    "train_keywords = pd.DataFrame(pd.get_dummies(train.keyword,prefix='KW'))\n",
    "train_tweets = pd.DataFrame(cv.transform(train['text']).toarray(),columns=cv.get_feature_names())\n",
    "X_train = pd.concat([train_keywords,train_tweets],axis=1)\n",
    "y_train = train.target\n",
    "\n",
    "test_keywords = pd.DataFrame(pd.get_dummies(test.keyword,prefix='KW'))\n",
    "test_tweets = pd.DataFrame(cv.transform(test['text']).toarray(),columns=cv.get_feature_names())\n",
    "X_test = pd.concat([test_keywords,test_tweets],axis=1)\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Ridge Classifier\n",
    "Let's start out with a very simple model: a ridge classifier. How well do we do for classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "model = RidgeClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4342\n",
      "           1       1.00      0.99      1.00      3271\n",
      "\n",
      "    accuracy                           1.00      7613\n",
      "   macro avg       1.00      1.00      1.00      7613\n",
      "weighted avg       1.00      1.00      1.00      7613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, nice! Nearly perfect precision and recall! I wasn't expecting that using default parameters. Looks like the data contain useful information for classifying tweets. It makes sense, of course, the data were labeled by human readers who looked at the same text information. They must have selected tweets that they were confident in classifying. \n",
    "\n",
    "### Dropping the keyword data\n",
    "\n",
    "Let's see if the results look quite as good if we drop the keyword data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      4342\n",
      "           1       1.00      0.99      0.99      3271\n",
      "\n",
      "    accuracy                           1.00      7613\n",
      "   macro avg       1.00      1.00      1.00      7613\n",
      "weighted avg       1.00      1.00      1.00      7613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = RidgeClassifier()\n",
    "model2.fit(cv.transform(train['text']).toarray(),y_train)\n",
    "y_pred = model2.predict(cv.transform(test['text']).toarray())\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers look a little worse, but keywords clearly don't have a drastic effect on the classification performance.\n",
    "\n",
    "# Unsupervised methods\n",
    "\n",
    "Now we can ask the question, why does it work so well? Here we can look to the patterns in the data using unsupervised methods. \n",
    "\n",
    "Can we turn the problem around and predict the keyword from the tweet text? If we did a topic analysis, would the topics map to keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: the that by was are with world has about at after still not last first more it's out we years\n",
      "Topic #1: ? # why follow so la airlines missing aircraft debris reunion _ found no u mh370 crush malaysia KW_crush this\n",
      "Topic #2: . with s it will not was & no u be p we have they it's o he KW_detonate m\n",
      "Topic #3: : rt 2015-08- รป_ california from as at 05 [ ] pm news s utc train police over 3 (\n",
      "Topic #4: a like was by at up get this but video with from what just after watch that under @youtube sandstorm\n",
      "Topic #5: ' from as are families wreckage by KW_wreckage confirmed who conclusively | were those mh370 malaysia ) it's video rescuers\n",
      "Topic #6: ! be & all out what please we news from wind ass check day KW_loud%20bang bang just loud hey neighbour's\n",
      "Topic #7: to be with going have get or make want go how as do it out over not we back so\n",
      "Topic #8: in killed suicide / crash people bomber who police up fire KW_hostages accident land two hostages released as are bomb\n",
      "Topic #9: i it was so but that me like just don't have am be know if got think when feel can't\n",
      "Topic #10: of an from hiroshima / out one atomic no anniversary families has bombing & with what that more than people\n",
      "Topic #11: - ( ) * by at with full / new video 11 news @youtube year-old charged via boy manslaughter read\n",
      "Topic #12: and it me with her like so we when was your all out will then just at they back had\n",
      "Topic #13: ย รป_ รป รปยชs รท รปยช ย full it รปรฒ [ รปยชt ] by at this รปรณ rt re new\n",
      "Topic #14: you your are if have that with when do can know don't me will been like so all i'm this\n",
      "Topic #15: is that it this he so one from there now but / who not if why about bringing no be\n",
      "Topic #16: ... by families over at legionnaires after from than fatal more has outbreak KW_outbreak 40 sue affected who #news pm\n",
      "Topic #17: on fire at buildings been that KW_buildings%20on%20fire / debris found reunion island your its were experts accident * KW_fire &\n",
      "Topic #18: for disaster obama declares typhoon-devastated saipan signs declaration northern KW_typhoon marians ( ) no rescuers KW_devastated us video KW_rescuers warning\n",
      "Topic #19: my me with just are & this up i'm into out loud ass all phone pick KW_loud%20bang bang car like\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "nmf = NMF(n_components=20, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5)\n",
    "\n",
    "nmf.fit(X_train,y_train)\n",
    "\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(nmf,X_train.columns,n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
